{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa05729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.stats.contingency_tables as ctt\n",
    "import numpy as np\n",
    "\n",
    "mistral_c = pd.read_csv('commonsense/mistral_500.csv')\n",
    "mistral_d = pd.read_csv('deontology/mistral_500.csv')\n",
    "mistral_j = pd.read_csv('justice/mistral_500.csv')\n",
    "mistral_u = pd.read_csv('utilitarianism/mistral_500_2.csv')\n",
    "mistral_v = pd.read_csv('virtue/mistral_500.csv')\n",
    "\n",
    "chatgpt_c = pd.read_csv('commonsense/Common_final_test_chatgpt.csv')\n",
    "chatgpt_d = pd.read_csv('deontology/deontology_final_test_chatgpt.csv')\n",
    "chatgpt_j = pd.read_csv('justice/justice_final_test_chatgpt.csv')\n",
    "chatgpt_u = pd.read_csv('utilitarianism/util_final_test_chatgpt.csv')\n",
    "chatgpt_v = pd.read_csv('virtue/virtue_final_test_chatgpt_cleaned.csv')\n",
    "\n",
    "gemini_c = pd.read_csv('commonsense/gemini_500.csv')\n",
    "gemini_d = pd.read_csv('deontology/gemini_500.csv')\n",
    "gemini_j = pd.read_csv('justice/gemini_500.csv')\n",
    "gemini_u = pd.read_csv('utilitarianism/gemini_500.csv')\n",
    "gemini_v = pd.read_csv('virtue/gemini_500.csv')\n",
    "\n",
    "answers_c = pd.read_csv('commonsense/cm_test_hard_answers.csv')\n",
    "answers_d = pd.read_csv('deontology/deontology_test_hard_answers_500.csv')\n",
    "answers_j = pd.read_csv('justice/justice_test_hard_answers_500.csv')\n",
    "answers_u = pd.read_csv('utilitarianism/util_test_hard_answers_500.csv')\n",
    "answers_v = pd.read_csv('virtue/virtue_test_hard_shuffled_answers.csv')\n",
    "\n",
    "pos = 1\n",
    "neg = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb8004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contingency_table(model1, model2, answers):\n",
    "\n",
    "    table = np.zeros((2, 2), dtype=int)\n",
    "    for i in range(len(model1)):\n",
    "        if answers.iloc[i, 0] == pos:\n",
    "            if model1.iloc[i, 0] == pos:\n",
    "                if model2.iloc[i, 0] == pos:\n",
    "                    table[0][0] += 1\n",
    "                else:\n",
    "                    table[0][1] += 1\n",
    "            else:\n",
    "                if model2.iloc[i, 0] == pos:\n",
    "                    table[1][0] += 1\n",
    "                else:\n",
    "                    table[1][1] += 1\n",
    "        else:\n",
    "            if model1.iloc[i, 0] == neg:\n",
    "                if model2.iloc[i, 0] == neg:\n",
    "                    table[0][0] += 1\n",
    "                else:\n",
    "                    table[0][1] += 1\n",
    "            else:\n",
    "                if model2.iloc[i, 0] == neg:\n",
    "                    table[1][0] += 1\n",
    "                else:\n",
    "                    table[1][1] += 1\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b48b14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contingency_table_nonbinary(model1, model2, answers):\n",
    "\n",
    "    table = np.zeros((2, 2), dtype=int)\n",
    "    for i in range(len(model1)):\n",
    "        if model1.iloc[i, 0] == answers.iloc[i, 0]:\n",
    "            if model2.iloc[i, 0] == answers.iloc[i, 0]:\n",
    "                table[0][0] += 1\n",
    "            else:\n",
    "                table[0][1] += 1\n",
    "        else:\n",
    "            if model2.iloc[i, 0] == answers.iloc[i, 0]:\n",
    "                table[1][0] += 1\n",
    "            else:\n",
    "                table[1][1] += 1\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06d9ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size(table):\n",
    "    return ((table[0][1]/500 + table[1][0]/500) * (1.96 + 0.8) ** 2) / (table[1][0]/500 - table[0][1]/500) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "040013f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency table for Mistral vs ChatGPT ([[449  21]\n",
      " [ 18  12]]):\n",
      "16505\n",
      "Contingency table for Mistral vs Gemini ([[463   7]\n",
      " [ 22   8]]):\n",
      "491\n",
      "Contingency table for ChatGPT vs Gemini ([[458   9]\n",
      " [ 27   6]]):\n",
      "424\n",
      "Contingency table for Mistral vs ChatGPT ([[286 105]\n",
      " [ 34  75]]):\n",
      "106\n",
      "Contingency table for Mistral vs Gemini ([[361  30]\n",
      " [ 53  56]]):\n",
      "598\n",
      "Contingency table for ChatGPT vs Gemini ([[292  28]\n",
      " [122  58]]):\n",
      "65\n",
      "Contingency table for Mistral vs ChatGPT ([[282 154]\n",
      " [ 41  23]]):\n",
      "59\n",
      "Contingency table for Mistral vs Gemini ([[405  31]\n",
      " [ 46  18]]):\n",
      "1304\n",
      "Contingency table for ChatGPT vs Gemini ([[297  26]\n",
      " [154  23]]):\n",
      "42\n",
      "Contingency table for Mistral vs ChatGPT ([[245  43]\n",
      " [ 99 113]]):\n",
      "173\n",
      "Contingency table for Mistral vs Gemini ([[243  45]\n",
      " [ 58 154]]):\n",
      "2322\n",
      "Contingency table for ChatGPT vs Gemini ([[263  81]\n",
      " [ 38 118]]):\n",
      "246\n",
      "Contingency table for Mistral vs ChatGPT ([[312  78]\n",
      " [ 35  75]]):\n",
      "233\n",
      "Contingency table for Mistral vs Gemini ([[373  17]\n",
      " [ 61  49]]):\n",
      "154\n",
      "Contingency table for ChatGPT vs Gemini ([[329  18]\n",
      " [105  48]]):\n",
      "62\n",
      "{'c': {'mistral': {'mistral': 1.0, 'chat': 16504.799999999967, 'gemini': 490.9119999999999}, 'chat': {'chat': 1.0, 'mistral': 16504.799999999967, 'gemini': 423.19999999999976}, 'gemini': {'gemini': 1.0, 'mistral': 490.9119999999999, 'chat': 423.19999999999976}}, 'd': {'mistral': {'mistral': 1.0, 'chat': 105.02344772862529, 'gemini': 597.5999999999999}, 'chat': {'chat': 1.0, 'mistral': 105.02344772862529, 'gemini': 64.65821638750563}, 'gemini': {'gemini': 1.0, 'mistral': 597.5999999999999, 'chat': 64.65821638750563}}, 'j': {'mistral': {'mistral': 1.0, 'chat': 58.16555720886523, 'gemini': 1303.4559999999997}, 'chat': {'chat': 1.0, 'mistral': 58.16555720886523, 'gemini': 41.84472656249999}, 'gemini': {'gemini': 1.0, 'mistral': 1303.4559999999997, 'chat': 41.84472656249999}}, 'u': {'mistral': {'mistral': 1.0, 'chat': 172.4647959183673, 'gemini': 2321.3396449704123}, 'chat': {'chat': 1.0, 'mistral': 172.4647959183673, 'gemini': 245.13098972417512}, 'gemini': {'gemini': 1.0, 'mistral': 2321.3396449704123, 'chat': 245.13098972417512}}, 'v': {'mistral': {'mistral': 1.0, 'chat': 232.77144402379665, 'gemini': 153.45371900826444}, 'chat': {'chat': 1.0, 'mistral': 232.77144402379665, 'gemini': 61.894887039238995}, 'gemini': {'gemini': 1.0, 'mistral': 153.45371900826444, 'chat': 61.894887039238995}}}\n"
     ]
    }
   ],
   "source": [
    "import math as Math\n",
    "sample_sizes = {'c': {'mistral':{}, 'chat':{}, 'gemini':{}}, \n",
    "            'd': {'mistral':{}, 'chat':{}, 'gemini':{}}, \n",
    "            'j': {'mistral':{}, 'chat':{}, 'gemini':{}}, \n",
    "            'u': {'mistral':{}, 'chat':{}, 'gemini':{}}, \n",
    "            'v': {'mistral':{}, 'chat':{}, 'gemini':{}}}\n",
    "\n",
    "for i in ['c', 'd', 'j', 'u', 'v']:\n",
    "    sample_sizes[i]['mistral']['mistral'] = 1.0\n",
    "    sample_sizes[i]['chat']['chat'] = 1.0\n",
    "    sample_sizes[i]['gemini']['gemini'] = 1.0\n",
    "\n",
    "for i in ['c', 'd', 'j', 'u', 'v']:\n",
    "    if not i == 'v':\n",
    "        mistral = globals()[f\"mistral_{i}\"]\n",
    "        chatgpt = globals()[f\"chatgpt_{i}\"]\n",
    "        gemini = globals()[f\"gemini_{i}\"]\n",
    "        answers = globals()[f\"answers_{i}\"]\n",
    "        table1 = np.array(get_contingency_table(mistral, chatgpt, answers))\n",
    "        table2 = np.array(get_contingency_table(mistral, gemini, answers))\n",
    "        table3 = np.array(get_contingency_table(chatgpt, gemini, answers))\n",
    "    else:\n",
    "        mistral = globals()[f\"mistral_{i}\"]\n",
    "        chatgpt = globals()[f\"chatgpt_{i}\"]\n",
    "        gemini = globals()[f\"gemini_{i}\"]\n",
    "        answers = globals()[f\"answers_{i}\"]\n",
    "        table1 = np.array(get_contingency_table_nonbinary(mistral, chatgpt, answers))\n",
    "        table2 = np.array(get_contingency_table_nonbinary(mistral, gemini, answers))\n",
    "        table3 = np.array(get_contingency_table_nonbinary(chatgpt, gemini, answers))\n",
    "\n",
    "\n",
    "    tables = [table1, table2, table3]\n",
    "    labels = [\n",
    "        \"Contingency table for Mistral vs ChatGPT\",\n",
    "        \"Contingency table for Mistral vs Gemini\",\n",
    "        \"Contingency table for ChatGPT vs Gemini\"\n",
    "    ]\n",
    "    for idx, tbl in enumerate(tables):\n",
    "        print(f\"{labels[idx]} ({tbl}):\")\n",
    "\n",
    "        if idx == 0:\n",
    "            sample_sizes[i]['mistral']['chat'] = sample_size(tbl)\n",
    "            sample_sizes[i]['chat']['mistral'] = sample_size(tbl)\n",
    "            print(Math.ceil(sample_sizes[i]['mistral']['chat']))\n",
    "        elif idx == 1:\n",
    "            sample_sizes[i]['mistral']['gemini'] = sample_size(tbl)\n",
    "            sample_sizes[i]['gemini']['mistral'] = sample_size(tbl)\n",
    "            print(Math.ceil(sample_sizes[i]['mistral']['gemini']))\n",
    "        elif idx == 2:\n",
    "            sample_sizes[i]['chat']['gemini'] = sample_size(tbl)\n",
    "            sample_sizes[i]['gemini']['chat'] = sample_size(tbl)\n",
    "            print(Math.ceil(sample_sizes[i]['chat']['gemini']))\n",
    "\n",
    "print(sample_sizes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
